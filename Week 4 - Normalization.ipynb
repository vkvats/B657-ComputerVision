{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMMlblEX3d/OnNPKjS51S5V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["## Imports\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math"],"metadata":{"id":"B-0aWYF-9JD_","executionInfo":{"status":"ok","timestamp":1738690579788,"user_tz":300,"elapsed":87,"user":{"displayName":"Vibhas Vats","userId":"13570104701153014775"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["## Local Response Normalization"],"metadata":{"id":"7wrJQxNo8uhp"}},{"cell_type":"code","source":["# Local Response Normalization (LRN) is a normalization layer that\n","# operates on local input regions within a feature map.  It's less\n","# common now, often superseded by batch normalization, but it can still\n","# be useful in specific situations.\n","\n","# When to use LRN:\n","# 1.  Historically, LRN was used to enhance the generalization ability\n","#     of convolutional neural networks (CNNs), particularly in the\n","#     AlexNet architecture.  It was thought to encourage competition\n","#     among neurons within the same feature map, promoting more robust\n","#     feature representations.\n","\n","# 2.  LRN can be beneficial when dealing with image data where local\n","#     contrast normalization is desired.  It normalizes responses\n","#     across adjacent channels, which can help to suppress background\n","#     noise and highlight salient features.\n","\n","\n","# When NOT to use LRN:\n","# 1. Batch Normalization (BatchNorm): BatchNorm has largely replaced LRN\n","#    because it's generally more effective in improving the training\n","#    process and model performance.  BatchNorm normalizes across the\n","#    batch dimension, providing a more stable and efficient way to\n","#    handle internal covariate shift.\n","\n","# 2.  Modern CNN Architectures:  Most state-of-the-art architectures\n","#     no longer include LRN layers.  The benefits of LRN are typically\n","#     outweighed by BatchNorm or other normalization techniques.\n","\n","# 3.  Small Datasets: LRN might be less effective on smaller datasets\n","#     because the normalization statistics calculated within local\n","#     regions might not be representative.\n","\n","class LocalResponseNorm2D(nn.Module):\n","    def __init__(self, size=5, alpha=1e-4, beta=0.75, k=2):\n","        super(LocalResponseNorm2D, self).__init__()\n","        self.size = size\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.k = k\n","\n","    def forward(self, x):\n","        return F.local_response_norm(x, size=self.size, alpha=self.alpha, beta=self.beta, k=self.k)\n","\n","## Example usage\n","batch_size = 32\n","channels = 3\n","height, width = 224, 224\n","\n","input_tensor = torch.randn(batch_size, channels, height, width)\n","\n","lrn_example = LocalResponseNorm2D()\n","lrn_output = lrn_example(input_tensor)\n","\n","print(\"Local Response Normalization output shape:\", lrn_output.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"28Nx_7jg8qYV","executionInfo":{"status":"ok","timestamp":1738690580178,"user_tz":300,"elapsed":314,"user":{"displayName":"Vibhas Vats","userId":"13570104701153014775"}},"outputId":"4e842f50-e0b2-4f54-e066-250ea466bf08"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Local Response Normalization output shape: torch.Size([32, 3, 224, 224])\n"]}]},{"cell_type":"markdown","source":["## Batch Normalization"],"metadata":{"id":"k-O_UFjq8z3Z"}},{"cell_type":"code","source":["# When to use:  Generally effective for larger batch sizes and when the distribution of activations\n","#               across the batch is representative of the overall data distribution.  Suitable for\n","#               most convolutional neural networks (CNNs) when training data is plentiful.\n","# How to use: Apply after a convolutional or fully connected layer.  The normalization is performed across\n","#              the mini-batch.\n","# When not to use:\n","\n","# 1. Small Batch Sizes: Batch Normalization calculates statistics (mean and variance) over a mini-batch.\n","#           With very small batch sizes, these statistics become unreliable and noisy, potentially harming the model's performance.\n","#           In such cases, consider Layer Normalization or Group Normalization which are less sensitive to batch size.\n","# 2. Recurrent Neural Networks (RNNs):  Applying Batch Normalization directly to RNNs can be problematic\n","#           due to the sequential nature of the data. The normalization statistics are calculated across different\n","#           sequences in the batch, which might not be meaningful.  Instead, consider using Layer Normalization\n","#           which normalizes across the features of a single time step.\n","# 3. Online Learning:  Batch Normalization relies on batch statistics. In online learning scenarios\n","#           where you receive and process data instances individually, it's not directly applicable.\n","# 4. Limited Computational Resources:  Batch Normalization adds computational overhead.\n","#           If you're working with limited resources, it might be a performance bottleneck.\n","# 5. When other normalization methods are more suitable:  There are other normalization\n","#           techniques like Layer Normalization, Instance Normalization, and Group Normalization\n","#           that may be more appropriate depending on the architecture and task.\n","#           Consider alternatives if Batch Normalization doesn't improve performance or adds undesirable effects.\n","\n","class BatchNormExample(nn.Module):\n","    def __init__(self, in_channels):\n","        super(BatchNormExample, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n","        self.bn = nn.BatchNorm2d(64)  ## Applies batch norm to the 64 output channels\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.bn(x)\n","        return x\n","\n","## Example usage\n","batch_size = 32\n","channels = 3\n","height, width = 224, 224\n","\n","input_tensor = torch.randn(batch_size, channels, height, width)\n","\n","bn_example = BatchNormExample(channels)\n","bn_output = bn_example(input_tensor)\n","\n","print(\"Batch Norm output shape:\", bn_output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mV4fnbrS72xU","executionInfo":{"status":"ok","timestamp":1738690581695,"user_tz":300,"elapsed":1518,"user":{"displayName":"Vibhas Vats","userId":"13570104701153014775"}},"outputId":"a5877cd8-bbe8-4a80-8a87-68343bf09b17"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch Norm output shape: torch.Size([32, 64, 224, 224])\n"]}]},{"cell_type":"markdown","source":["## Group Normalization"],"metadata":{"id":"5MdUIdE_83Am"}},{"cell_type":"code","source":["# When to use:  Useful when batch sizes are small, where batch normalization might not be as reliable due to\n","#               limited sample diversity.  Helps to stabilize training in situations with limited data.\n","# How to use:  Similar to Batch Normalization but divides the channels into groups and performs normalization\n","#              within each group.  The number of groups is a hyperparameter.\n","# When not to use:\n","#           Group Normalization (GN) is less effective than other normalization methods when the number of\n","#           channels in your data is small.  Since GN normalizes across groups of channels, if the number of\n","#           channels is small (e.g., less than the group size), the normalization statistics become less\n","#           reliable and may not properly capture the distribution of activations. In such scenarios,\n","#           other techniques like Layer Normalization might be preferable.\n","\n","class GroupNormExample(nn.Module):\n","    def __init__(self, in_channels):\n","        super(GroupNormExample, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n","        self.gn = nn.GroupNorm(num_groups=8, num_channels=64)  ## 8 groups\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.gn(x)\n","        return x\n","\n","## Example usage\n","batch_size = 32\n","channels = 3\n","height, width = 224, 224\n","\n","input_tensor = torch.randn(batch_size, channels, height, width)\n","\n","gn_example = GroupNormExample(channels)\n","gn_output = gn_example(input_tensor)\n","\n","print(\"Group Norm output shape:\", gn_output.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aP0Z0RM572oD","executionInfo":{"status":"ok","timestamp":1738690582713,"user_tz":300,"elapsed":1020,"user":{"displayName":"Vibhas Vats","userId":"13570104701153014775"}},"outputId":"e4732832-a4b7-421e-c3b0-47006606ed37"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Group Norm output shape: torch.Size([32, 64, 224, 224])\n"]}]},{"cell_type":"markdown","source":["## Instance Normalization"],"metadata":{"id":"5bJnBhuJ85v0"}},{"cell_type":"code","source":["## Instance Normalization\n","# When to use: Effective for style transfer and image generation tasks, particularly when the statistics\n","#              of each image should be normalized independently.\n","# How to use: Normalizes the activations of each channel separately for each image in the batch.\n","#              Useful when the style of each instance needs to be preserved, as it does not rely on\n","#              the statistics of other images.\n","# When not to use:\n","# 1. When batch statistics are important: Instance Normalization normalizes each channel of each individual image in the batch independently.\n","#           It ignores the information across the batch dimension.  If you need to use batch statistics for normalization (like in Batch Normalization),\n","#           then Instance Normalization is not a suitable substitute.\n","# 2. Tasks that benefit from inter-sample relationships:  Tasks like image classification often benefit from\n","#           learning features that are shared across images within a batch. Instance Normalization removes this information,\n","#           potentially hindering performance.  Since it normalizes per image, it removes the information\n","#           about how a specific feature varies *across* images in a batch.  For classification, this could be detrimental.\n","# 3. Small datasets or images: While the code doesn't explicitly mention it,  like other normalization methods,\n","#           Instance Normalization's effectiveness can be affected by small datasets or image sizes.\n","#           The statistics calculated per image might be unreliable in these scenarios.\n","# 4. Style transfer applications (sometimes): While Instance Normalization is often used in style transfer,\n","#           there are cases where other normalization methods might perform better. The choice depends on the\n","#           specifics of the model and the desired effect.  In style transfer, the goal is often to transfer\n","#           the style *across* images.  If you're not careful about how to apply Instance Norm,\n","#           it might end up isolating each image more than you want.\n","\n","class InstanceNormExample(nn.Module):\n","    def __init__(self, in_channels):\n","        super(InstanceNormExample, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n","        self.inorm = nn.InstanceNorm2d(64, affine=True) # affine=True allows learnable scaling and bias parameters\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.inorm(x)\n","        return x\n","\n","## Example usage\n","batch_size = 32\n","channels = 3\n","height, width = 224, 224\n","\n","input_tensor = torch.randn(batch_size, channels, height, width)\n","\n","inorm_example = InstanceNormExample(channels)\n","inorm_output = inorm_example(input_tensor)\n","\n","print(\"Instance Norm output shape:\", inorm_output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3BL8810d72gq","executionInfo":{"status":"ok","timestamp":1738690584108,"user_tz":300,"elapsed":1397,"user":{"displayName":"Vibhas Vats","userId":"13570104701153014775"}},"outputId":"d4250c93-e601-4da9-a57e-5dccf89531f6"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Instance Norm output shape: torch.Size([32, 64, 224, 224])\n"]}]},{"cell_type":"markdown","source":["## Layer Normalization"],"metadata":{"id":"eo9tMgkK8-Rv"}},{"cell_type":"code","source":["# When to use:  Effective when batch sizes are very small or when you want to normalize activations\n","#               across features within a single sample, irrespective of the batch.  Often used in\n","#               recurrent neural networks (RNNs) and transformers.\n","# How to use:  Normalizes the activations across the features (channels) for each element in the sequence.\n","#              It does not depend on the batch statistics, making it suitable for situations with variable-length\n","#              sequences or small batch sizes.\n","# When NOT to use:\n","# 1. Convolutional Neural Networks (CNNs): Layer Normalization is generally less effective than\n","#    Batch Normalization in CNNs.  BatchNorm's normalization across the batch dimension often leads to\n","#    better performance in image-related tasks.  Layer Normalization normalizes across channels, which might\n","#    not capture the relevant statistical properties for image data as effectively.\n","\n","# 2. Recurrent Neural Networks (RNNs) with large sequence lengths: While Layer Normalization is often\n","#    preferred over Batch Normalization in RNNs due to the varying sequence lengths, it can still be less\n","#    effective than other methods like RMSNorm when dealing with very long sequences. The normalization\n","#    across the feature dimension might not capture the temporal dependencies as well in this case.\n","\n","# 3. When batch statistics are crucial:  If the statistical properties across the batch are important\n","#    for the model's learning (e.g., in some generative models), Layer Normalization, which ignores the\n","#    batch dimension, may not be the best choice.  In this case, Batch Normalization is more appropriate.\n","\n","# 4. When small batch sizes are not a concern: The primary advantage of Layer Normalization over\n","#     Batch Normalization is its robustness to small batch sizes.  If you have reasonably large batch sizes,\n","#     then Batch Normalization often performs better.\n","\n","# 5. When other normalization methods are more suitable:  Always consider other alternatives like\n","#     Group Normalization, Instance Normalization, or even techniques like Weight Standardization.\n","#     Experimentation is key to determine the best normalization technique for a given architecture and dataset.\n","\n","class LayerNormExample(nn.Module):\n","    def __init__(self, in_features):\n","        super(LayerNormExample, self).__init__()\n","        self.linear = nn.Linear(in_features, 64)\n","        self.ln = nn.LayerNorm(64) ## Applies layer norm to the 64 output features\n","\n","    def forward(self, x):\n","        x = self.linear(x)\n","        x = self.ln(x)\n","        return x\n","\n","## Example usage (assuming input is a sequence of vectors)\n","sequence_length = 10\n","in_features = 512\n","\n","input_sequence = torch.randn(batch_size, sequence_length, in_features)\n","\n","ln_example = LayerNormExample(in_features)\n","ln_output = ln_example(input_sequence)\n","\n","print(\"Layer Norm output shape:\", ln_output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PJcczAJV9Af4","executionInfo":{"status":"ok","timestamp":1738690584108,"user_tz":300,"elapsed":3,"user":{"displayName":"Vibhas Vats","userId":"13570104701153014775"}},"outputId":"4a2fab33-11b5-4b4d-92d4-7ffd25e7a332"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer Norm output shape: torch.Size([32, 10, 64])\n"]}]},{"cell_type":"markdown","source":["## Weight Standardization"],"metadata":{"id":"7pxXhNgKAZoF"}},{"cell_type":"code","source":["# When to use: Weight standardization can be beneficial when you want to stabilize the training process\n","#              of deep neural networks, especially when dealing with very deep or complex architectures.\n","#              It can sometimes provide a performance boost, especially in conjunction with other\n","#              regularization techniques like weight decay.\n","# How to use:  Apply weight standardization to the weights of each layer.  It's typically done within the\n","#              layer's forward pass before applying the weights to the input activations.\n","# When not to use:\n","# 1. Shallow Networks:  For relatively shallow networks, the benefits of weight standardization might be\n","#                      minimal or even negligible. Other normalization techniques like BatchNorm might be\n","#                      more effective.\n","# 2. Already Stable Training: If your network's training process is already stable (e.g., using BatchNorm),\n","#                            introducing weight standardization might not provide additional benefits and\n","#                            could even slightly hurt performance.\n","# 3. Computational Constraints: Weight standardization adds a small amount of computational overhead. If\n","#                              you have limited computational resources, this might be a concern.\n","# 4. Hyperparameter Tuning:  As with any normalization technique, tuning the learning rate and weight decay\n","#                           may be necessary when using Weight Standardization.\n","\n","class WeightStandardization(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n","        super(WeightStandardization, self).__init__()\n","        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size))\n","        self.bias = nn.Parameter(torch.Tensor(out_channels))\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5)) ## You can use other initializations\n","        if self.bias is not None:\n","            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n","            bound = 1 / math.sqrt(fan_in)\n","            nn.init.uniform_(self.bias, -bound, bound)\n","\n","\n","    def forward(self, x):\n","        ## Weight standardization\n","        mean = self.weight.mean(dim=[1, 2, 3], keepdim=True)\n","        std = self.weight.std(dim=[1, 2, 3], keepdim=True) + 1e-5  ## Add small epsilon for stability\n","        normalized_weight = (self.weight - mean) / std\n","\n","        ## Convolution\n","        return F.conv2d(x, normalized_weight, self.bias, padding=1)\n","\n","\n","## Example Usage\n","batch_size = 32\n","channels = 3\n","height, width = 224, 224\n","\n","input_tensor = torch.randn(batch_size, channels, height, width)\n","\n","ws_example = WeightStandardization(channels, 64)\n","ws_output = ws_example(input_tensor)\n","\n","print(\"Weight Standardization output shape:\", ws_output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tu44Qrv6A50i","executionInfo":{"status":"ok","timestamp":1738690584902,"user_tz":300,"elapsed":796,"user":{"displayName":"Vibhas Vats","userId":"13570104701153014775"}},"outputId":"dd0731c9-5ca7-4e06-aabf-7c1025cbbb24"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Weight Standardization output shape: torch.Size([32, 64, 224, 224])\n"]}]},{"cell_type":"markdown","source":["## Drop-out Layer"],"metadata":{"id":"O6KpLUAWBJZ2"}},{"cell_type":"code","source":["## When to use Dropout:\n","# 1.  Prevent Overfitting: Dropout is primarily used to reduce overfitting in neural networks.\n","#     It randomly deactivates neurons during training, forcing the network to learn more robust\n","#     and generalized features.  It's particularly effective when you have a limited amount of training data.\n","# 2.  Complex Architectures:  Dropout is often beneficial in deep or complex networks with many layers\n","#     and parameters, as these architectures are more prone to overfitting.\n","\n","## How to use Dropout:\n","# 1.  Insert Dropout Layers: Add dropout layers strategically within your network architecture, typically\n","#     between fully connected layers or convolutional layers.\n","# 2.  Dropout Rate:  The dropout rate (p) is a hyperparameter that controls the probability of a neuron being deactivated.\n","#     Typical values are between 0.2 and 0.5. A higher dropout rate means more neurons are deactivated.\n","#     Experiment to find the best value for your specific task.\n","# 3.  Training vs. Inference:  It is crucial to remember that dropout is only active during the training phase.\n","#     During inference (when you use the model for prediction), dropout should be turned off.  Most deep learning\n","#     frameworks handle this automatically when you set the model to \"eval\" mode.\n","\n","\n","## When NOT to use Dropout:\n","# 1.  Small Networks: For very small networks with few parameters, dropout might not be necessary and could even\n","#     harm performance.  Overfitting is less of a concern in these cases.\n","# 2.  Limited Training Data: While dropout helps with limited data, if your dataset is extremely small,\n","#     dropout might exacerbate the problem of insufficient data. Other regularization techniques might be better\n","#     suited in this case.\n","# 3.  When other regularization techniques are more effective: Data augmentation and weight decay are often\n","#     very effective regularization methods. In some cases, they might be sufficient, and dropout could be redundant.\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(784, 256)\n","        self.dropout1 = nn.Dropout(0.2)  ## Dropout with 20% probability\n","        self.fc2 = nn.Linear(256, 128)\n","        self.dropout2 = nn.Dropout(0.3) ## Dropout with 30% probability\n","        self.fc3 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 784) ## Flatten the input image\n","        x = torch.relu(self.fc1(x))\n","        x = self.dropout1(x)\n","        x = torch.relu(self.fc2(x))\n","        x = self.dropout2(x)\n","        x = self.fc3(x)\n","        return x\n","\n","## Example usage:\n","## Create an instance of the network\n","net = Net()\n","\n","## Create a sample input tensor\n","input_tensor = torch.randn(1, 784)  # Example input (batch size 1, 784 features)\n","\n","## Set the model to training mode (dropout is active)\n","net.train()\n","\n","## Perform a forward pass\n","output = net(input_tensor)\n","\n","## Print the output shape\n","print(\"Output shape (Training mode):\", output.shape)\n","\n","## Set the model to evaluation mode (dropout is inactive)\n","net.eval()\n","\n","## Perform a forward pass in evaluation mode\n","with torch.no_grad(): ## No need to calculate the gradient during evaluation\n","    output_eval = net(input_tensor)\n","\n","## Print the output shape (evaluation mode)\n","print(\"Output shape (Evaluation mode):\", output_eval.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bhguSNHOBmk6","executionInfo":{"status":"ok","timestamp":1738690584903,"user_tz":300,"elapsed":3,"user":{"displayName":"Vibhas Vats","userId":"13570104701153014775"}},"outputId":"0f5d46f1-6a21-4e8a-ad32-68b0be69241b"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Output shape (Training mode): torch.Size([1, 10])\n","Output shape (Evaluation mode): torch.Size([1, 10])\n"]}]}]}
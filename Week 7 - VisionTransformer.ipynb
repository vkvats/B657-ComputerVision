{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMcaig+AjQEpvwNma6+BnKe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["## Imports\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","from torchvision import models\n","import matplotlib.pyplot as plt\n","import numpy as np"],"metadata":{"id":"vXY3pC-Zy5Kf","executionInfo":{"status":"ok","timestamp":1739458331482,"user_tz":300,"elapsed":161,"user":{"displayName":"Vibhas Vats","userId":"13570104701153014775"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Vision Transformer (ViT) Implementation\n","\n","class PatchEmbedding(nn.Module):\n","    \"\"\"Converts an image into a sequence of flattened patches.\"\"\"\n","    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n","        super().__init__()\n","        self.patch_size = patch_size\n","        self.num_patches = (img_size // patch_size) ** 2\n","        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n","\n","    def forward(self, x):\n","        x = self.proj(x).flatten(2).transpose(1, 2)  # (B, embed_dim, H*W) -> (B, H*W, embed_dim)\n","        return x\n","\n","class MultiHeadSelfAttention(nn.Module):\n","    \"\"\"Multi-Head Self-Attention mechanism for the Transformer Encoder.\"\"\"\n","    def __init__(self, embed_dim, num_heads):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","        assert self.head_dim * num_heads == embed_dim, \"Embedding size must be divisible by num_heads\"\n","\n","        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n","        self.fc_out = nn.Linear(embed_dim, embed_dim)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n","        queries, keys, values = qkv[0], qkv[1], qkv[2]\n","\n","        attention = (queries @ keys.transpose(-2, -1)) / np.sqrt(self.head_dim)\n","        attention = F.softmax(attention, dim=-1)\n","\n","        out = (attention @ values).transpose(1, 2).reshape(B, N, C)\n","        out = self.fc_out(out)\n","        return out, attention"],"metadata":{"id":"-yqfuWbvy5IV","executionInfo":{"status":"ok","timestamp":1739458345621,"user_tz":300,"elapsed":136,"user":{"displayName":"Vibhas Vats","userId":"13570104701153014775"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoderBlock(nn.Module):\n","    \"\"\"Single Transformer Encoder Block.\"\"\"\n","    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n","        super().__init__()\n","        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n","        self.norm1 = nn.LayerNorm(embed_dim)\n","        self.norm2 = nn.LayerNorm(embed_dim)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(embed_dim, mlp_dim),\n","            nn.GELU(),\n","            nn.Linear(mlp_dim, embed_dim),\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        attn_out, attn_weights = self.attn(x)\n","        x = self.norm1(x + attn_out)\n","        mlp_out = self.mlp(x)\n","        x = self.norm2(x + self.dropout(mlp_out))\n","        return x, attn_weights\n","\n","class VisionTransformer(nn.Module):\n","    \"\"\"Vision Transformer (ViT) Model.\"\"\"\n","    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, num_heads=8, mlp_dim=2048, num_layers=6, num_classes=10):\n","        super().__init__()\n","        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n","        self.pos_embedding = nn.Parameter(torch.randn(1, (img_size // patch_size) ** 2 + 1, embed_dim))\n","        self.encoder_layers = nn.ModuleList([\n","            TransformerEncoderBlock(embed_dim, num_heads, mlp_dim) for _ in range(num_layers)\n","        ])\n","        self.norm = nn.LayerNorm(embed_dim)\n","        self.head = nn.Linear(embed_dim, num_classes)\n","\n","    def forward(self, x):\n","        B = x.shape[0]\n","        x = self.patch_embed(x)\n","        cls_tokens = self.cls_token.expand(B, -1, -1)\n","        x = torch.cat([cls_tokens, x], dim=1)\n","        x += self.pos_embedding\n","\n","        attention_maps = []\n","        for layer in self.encoder_layers:\n","            x, attn_weights = layer(x)\n","            attention_maps.append(attn_weights)\n","\n","        x = self.norm(x[:, 0])\n","        return self.head(x), attention_maps"],"metadata":{"id":"EUlJy3DYy5FL","executionInfo":{"status":"ok","timestamp":1739458355647,"user_tz":300,"elapsed":116,"user":{"displayName":"Vibhas Vats","userId":"13570104701153014775"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Test Example\n","img_size = 224\n","patch_size = 16\n","model = VisionTransformer(img_size=img_size, patch_size=patch_size, num_classes=10)\n","test_input = torch.randn(1, 3, img_size, img_size)  # Simulated image input\n","output, attn_maps = model(test_input)\n","print(\"Test Output Shape:\", output.shape)\n","print(\"Attention Map Shape:\", attn_maps[0].shape)  # Display first layer attention map shape\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pvmgI5cdxMTz","executionInfo":{"status":"ok","timestamp":1739458361820,"user_tz":300,"elapsed":867,"user":{"displayName":"Vibhas Vats","userId":"13570104701153014775"}},"outputId":"1864d83a-c525-489e-cb9b-781971e33f27"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Output Shape: torch.Size([1, 10])\n","Attention Map Shape: torch.Size([1, 8, 197, 197])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"a8VAj0ZjzDXV"},"execution_count":null,"outputs":[]}]}